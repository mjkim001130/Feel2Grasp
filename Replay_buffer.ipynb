{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5324a7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 53936\n",
      "len(ds) = 53936\n",
      "ds.meta = LeRobotDatasetMetadata({\n",
      "    Repository ID: 'mjkim00/Feel2Grasp',\n",
      "    Total episodes: '200',\n",
      "    Total frames: '53936',\n",
      "    Features: '['action', 'observation.state', 'observation.images.front', 'observation.images.left', 'observation.images.right', 'timestamp', 'frame_index', 'episode_index', 'index', 'task_index']',\n",
      "})',\n",
      "\n",
      "encoded 256/53936\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 160\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, N, batch_size):\n\u001b[1;32m    159\u001b[0m     end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(N, start \u001b[38;5;241m+\u001b[39m batch_size)\n\u001b[0;32m--> 160\u001b[0m     Z[start:end] \u001b[38;5;241m=\u001b[39m \u001b[43mencode_front_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (start \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/so101/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 133\u001b[0m, in \u001b[0;36mencode_front_batch\u001b[0;34m(ds, idx_batch)\u001b[0m\n\u001b[1;32m    131\u001b[0m imgs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx_batch:\n\u001b[0;32m--> 133\u001b[0m     samp \u001b[38;5;241m=\u001b[39m \u001b[43mget_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     img \u001b[38;5;241m=\u001b[39m get_key(samp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation.images.front\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# torch.Tensor or np.ndarray\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# (3,H,W) float32 [0,1]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 91\u001b[0m, in \u001b[0;36mget_sample\u001b[0;34m(ds, i)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_sample\u001b[39m(ds, i: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m---> 91\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\n",
      "File \u001b[0;32m~/lerobot/src/lerobot/datasets/lerobot_dataset.py:1042\u001b[0m, in \u001b[0;36mLeRobotDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     current_ts \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1041\u001b[0m     query_timestamps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_query_timestamps(current_ts, query_indices)\n\u001b[0;32m-> 1042\u001b[0m     video_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_timestamps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mep_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m     item \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvideo_frames, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mitem}\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_transforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/lerobot/src/lerobot/datasets/lerobot_dataset.py:1007\u001b[0m, in \u001b[0;36mLeRobotDataset._query_videos\u001b[0;34m(self, query_timestamps, ep_idx)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     shifted_query_ts \u001b[38;5;241m=\u001b[39m [from_timestamp \u001b[38;5;241m+\u001b[39m ts \u001b[38;5;28;01mfor\u001b[39;00m ts \u001b[38;5;129;01min\u001b[39;00m query_ts]\n\u001b[1;32m   1006\u001b[0m     video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mget_video_file_path(ep_idx, vid_key)\n\u001b[0;32m-> 1007\u001b[0m     frames \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_video_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshifted_query_ts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolerance_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_backend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1008\u001b[0m     item[vid_key] \u001b[38;5;241m=\u001b[39m frames\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "File \u001b[0;32m~/lerobot/src/lerobot/datasets/video_utils.py:71\u001b[0m, in \u001b[0;36mdecode_video_frames\u001b[0;34m(video_path, timestamps, tolerance_s, backend)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decode_video_frames_torchcodec(video_path, timestamps, tolerance_s)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyav\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_reader\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecode_video_frames_torchvision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported video backend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/lerobot/src/lerobot/datasets/video_utils.py:112\u001b[0m, in \u001b[0;36mdecode_video_frames_torchvision\u001b[0;34m(video_path, timestamps, tolerance_s, backend, log_loaded_timestamps)\u001b[0m\n\u001b[1;32m    108\u001b[0m     keyframes_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# pyav doesn't support accurate seek\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# set a video stream reader\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# TODO(rcadene): also load audio stream at the same time\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVideoReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvideo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# set the first and last requested timestamps\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Note: previous timestamps are usually loaded, since we need to access the previous key frame\u001b[39;00m\n\u001b[1;32m    116\u001b[0m first_ts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timestamps)\n",
      "File \u001b[0;32m~/miniconda3/envs/so101/lib/python3.10/site-packages/torchvision/io/video_reader.py:167\u001b[0m, in \u001b[0;36mVideoReader.__init__\u001b[0;34m(self, src, stream, num_threads)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39minit_from_memory(src, stream, num_threads)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyav\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[43mav\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# TODO: load metadata\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     stream_type \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from lerobot.datasets.lerobot_dataset import LeRobotDataset\n",
    "\n",
    "\n",
    "# ======================== Configuration ========================\n",
    "parquet_path = \"/home/joy4mj/Feel2Grasp/train.parquet\" \n",
    "repo_id = \"mjkim00/Feel2Grasp\"\n",
    "revision = \"main\"\n",
    "video_backend = \"pyav\"\n",
    "\n",
    "encoder_ckpt_path = \"./ae_out/encoder.pt\"\n",
    "batch_size = 256\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "out_path = \"replay_buffer_iql_72d.npz\"\n",
    "\n",
    "\n",
    "# Encoder\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, 2, 1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((4, 4)),\n",
    "        )\n",
    "        self.fc = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.net(x).flatten(1))\n",
    "\n",
    "ckpt = torch.load(encoder_ckpt_path, map_location=\"cpu\")\n",
    "encoder = ConvEncoder(ckpt[\"latent_dim\"])\n",
    "encoder.load_state_dict(ckpt[\"encoder_state_dict\"])\n",
    "encoder.to(device).eval()\n",
    "for p in encoder.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "resize_hw = tuple(ckpt[\"resize_hw\"])  # (H, W)\n",
    "\n",
    "# Parquet load\n",
    "df = pd.read_parquet(parquet_path)\n",
    "\n",
    "need_cols = [\n",
    "    \"action\", \"observation.state\", \"episode_index\", \"frame_index\", \"index\",\n",
    "    \"left_image_circle\", \"right_image_circle\", \"circle_reward\"\n",
    "]\n",
    "missing = [c for c in need_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing columns: {missing}\\nAvailable: {list(df.columns)}\")\n",
    "\n",
    "df = df.sort_values([\"episode_index\", \"frame_index\"]).reset_index(drop=True)\n",
    "\n",
    "N = len(df)\n",
    "print(\"N =\", N)\n",
    "\n",
    "ep = df[\"episode_index\"].to_numpy()\n",
    "fr = df[\"frame_index\"].to_numpy()\n",
    "global_idx = df[\"index\"].to_numpy()\n",
    "\n",
    "dup = df.duplicated([\"episode_index\", \"frame_index\"]).any()\n",
    "if dup:\n",
    "    raise ValueError(\"Duplicated (episode_index, frame_index) rows exist in parquet. Fix this first.\")\n",
    "\n",
    "# observation.state / action stack\n",
    "obs_state = np.stack(df[\"observation.state\"].to_list()).astype(np.float32)  # (N,6) \n",
    "actions   = np.stack(df[\"action\"].to_list()).astype(np.float32)            # (N,Da)\n",
    "lr01      = df[[\"left_image_circle\", \"right_image_circle\"]].to_numpy().astype(np.float32)  # (N,2)\n",
    "rewards   = df[\"circle_reward\"].to_numpy().astype(np.float32)              # (N,)\n",
    "\n",
    "if obs_state.ndim != 2 or obs_state.shape[1] != 6:\n",
    "    raise ValueError(f\"observation.state expected (N,6) but got {obs_state.shape}\")\n",
    "\n",
    "# reward rescaling\n",
    "rewards = rewards / 10.0\n",
    "\n",
    "\n",
    "# load LeRobotDataset 로드\n",
    "ds = LeRobotDataset(repo_id, revision=revision, video_backend=video_backend)\n",
    "print(\"len(ds) =\", len(ds))\n",
    "print(\"ds.meta =\", ds.meta)\n",
    "\n",
    "def get_sample(ds, i: int):\n",
    "    s = ds[int(i)]\n",
    "    return s\n",
    "\n",
    "def get_key(sample, key):\n",
    "    if key in sample:\n",
    "        return sample[key]\n",
    "    raise KeyError(f\"Key {key} not found in sample keys: {list(sample.keys())[:30]} ...\")\n",
    "\n",
    "check_k = min(200, N)\n",
    "ok_direct = True\n",
    "for k in np.linspace(0, N-1, check_k, dtype=int):\n",
    "    i = int(global_idx[k])\n",
    "    samp = get_sample(ds, i)\n",
    "    ep_ds = int(get_key(samp, \"episode_index\"))\n",
    "    fr_ds = int(get_key(samp, \"frame_index\"))\n",
    "    if ep_ds != int(ep[k]) or fr_ds != int(fr[k]):\n",
    "        ok_direct = False\n",
    "        break\n",
    "\n",
    "if not ok_direct:\n",
    "    print(\"[WARN] parquet.index != ds index mapping. Building (episode,frame)->ds_index map...\")\n",
    "    map_epfr_to_dsidx = {}\n",
    "    for i in range(len(ds)):\n",
    "        samp = get_sample(ds, i)\n",
    "        ep_i = int(get_key(samp, \"episode_index\"))\n",
    "        fr_i = int(get_key(samp, \"frame_index\"))\n",
    "        map_epfr_to_dsidx[(ep_i, fr_i)] = i\n",
    "\n",
    "    ds_indices = np.empty((N,), dtype=np.int64)\n",
    "    for k in range(N):\n",
    "        key = (int(ep[k]), int(fr[k]))\n",
    "        if key not in map_epfr_to_dsidx:\n",
    "            raise KeyError(f\"Cannot find ds index for (episode,frame)={key}\")\n",
    "        ds_indices[k] = map_epfr_to_dsidx[key]\n",
    "else:\n",
    "    ds_indices = global_idx.astype(np.int64)\n",
    "\n",
    "# front image -> encoder -> z(64) \n",
    "@torch.no_grad()\n",
    "def encode_front_batch(ds, idx_batch):\n",
    "    imgs = []\n",
    "    for i in idx_batch:\n",
    "        samp = get_sample(ds, int(i))\n",
    "        img = get_key(samp, \"observation.images.front\")  # torch.Tensor or np.ndarray\n",
    "        # (3,H,W) float32 [0,1]\n",
    "        if isinstance(img, np.ndarray):\n",
    "            if img.ndim == 3 and img.shape[-1] == 3:  # HWC\n",
    "                img = torch.from_numpy(img).permute(2, 0, 1)\n",
    "            else:\n",
    "                img = torch.from_numpy(img)\n",
    "        # torch tensor\n",
    "        if img.dtype == torch.uint8:\n",
    "            img = img.float() / 255.0\n",
    "        else:\n",
    "            img = img.float()\n",
    "            if img.max() > 1.5:\n",
    "                img = img / 255.0\n",
    "\n",
    "        imgs.append(img)\n",
    "\n",
    "    x = torch.stack(imgs, dim=0).to(device)  # (B,3,H,W)\n",
    "    x = F.interpolate(x, size=resize_hw, mode=\"bilinear\", align_corners=False)\n",
    "    z = encoder(x)  # (B,64)\n",
    "    return z.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "Z = np.empty((N, ckpt[\"latent_dim\"]), dtype=np.float32)\n",
    "\n",
    "for start in range(0, N, batch_size):\n",
    "    end = min(N, start + batch_size)\n",
    "    Z[start:end] = encode_front_batch(ds, ds_indices[start:end])\n",
    "    if (start // batch_size) % 20 == 0:\n",
    "        print(f\"encoded {end}/{N}\")\n",
    "\n",
    "# state = [z64, left01, right01, obs_state6]\n",
    "states = np.concatenate([Z, lr01, obs_state], axis=1).astype(np.float32)  # (N,72)\n",
    "if states.shape[1] != 72:\n",
    "    raise ValueError(f\"states dim expected 72 but got {states.shape}\")\n",
    "\n",
    "\n",
    "# done / next_state \n",
    "terminals = np.zeros((N,), dtype=np.float32)\n",
    "# t가 terminal이면: 다음 row가 (same ep & frame+1)가 아님\n",
    "cont = (ep[1:] == ep[:-1]) & (fr[1:] == fr[:-1] + 1)\n",
    "terminals[:-1] = (~cont).astype(np.float32)\n",
    "terminals[-1] = 1.0\n",
    "\n",
    "next_states = np.empty_like(states)\n",
    "next_states[:-1] = states[1:]\n",
    "next_states[-1] = states[-1]\n",
    "\n",
    "terminal_idx = np.where(terminals > 0.5)[0]\n",
    "next_states[terminal_idx] = states[terminal_idx]\n",
    "\n",
    "\n",
    "np.savez_compressed(\n",
    "    out_path,\n",
    "    observations=states,\n",
    "    actions=actions.astype(np.float32),\n",
    "    rewards=rewards.astype(np.float32),\n",
    "    next_observations=next_states,\n",
    "    terminals=terminals,\n",
    "    episode_index=ep.astype(np.int32),\n",
    "    frame_index=fr.astype(np.int32),\n",
    "    ds_index=ds_indices.astype(np.int64),\n",
    ")\n",
    "\n",
    "print(\"Saved:\", out_path)\n",
    "print(\"Shapes:\",\n",
    "      \"S\", states.shape,\n",
    "      \"A\", actions.shape,\n",
    "      \"R\", rewards.shape,\n",
    "      \"S'\", next_states.shape,\n",
    "      \"D\", terminals.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf14b30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: episode/timestep alignment is consistent.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "d = np.load(\"replay_buffer_iql_72d.npz\")\n",
    "ep = d[\"episode_index\"]\n",
    "fr = d[\"frame_index\"]\n",
    "done = d[\"terminals\"]\n",
    "\n",
    "# check episode/timestep alignment\n",
    "idx = np.where(done[:-1] < 0.5)[0]\n",
    "assert np.all(ep[idx] == ep[idx+1]), \"Found done=0 but episode changes!\"\n",
    "assert np.all(fr[idx+1] == fr[idx] + 1), \"Found done=0 but frame_index not consecutive!\"\n",
    "\n",
    "# done==1 \n",
    "idx = np.where(done[:-1] > 0.5)[0]\n",
    "bad = np.where((ep[idx] == ep[idx+1]) & (fr[idx+1] == fr[idx] + 1))[0]\n",
    "assert len(bad) == 0, \"Found done=1 but next is still a valid continuation!\"\n",
    "\n",
    "print(\"OK: episode/timestep alignment is consistent.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c9372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 53936\n",
      "reward min/max: -0.1 1.0\n",
      "count success: 5932\n",
      "success ratio: 0.10998220112726194\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "d = np.load(\"replay_buffer_iql_normalized.npz\")\n",
    "r = d[\"rewards\"]\n",
    "print(\"N:\", len(r))\n",
    "print(\"reward min/max:\", r.min(), r.max())\n",
    "print(\"count success:\", (r > 0.9).sum())   \n",
    "print(\"success ratio:\", (r > 0.9).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d960403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "so101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
