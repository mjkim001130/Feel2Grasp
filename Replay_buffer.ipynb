{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad10ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 53936\n",
      "len(ds) = 53936\n",
      "encoded 256/53936\n",
      "encoded 5376/53936\n",
      "encoded 10496/53936\n",
      "encoded 15616/53936\n",
      "encoded 20736/53936\n",
      "encoded 25856/53936\n",
      "encoded 30976/53936\n",
      "encoded 36096/53936\n",
      "encoded 41216/53936\n",
      "encoded 46336/53936\n",
      "encoded 51456/53936\n",
      "Saved: replay_buffer_iql_normalized.npz\n",
      "Shapes: S (53936, 72) A (53936, 6) R (53936,) S' (53936, 72) D (53936,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from lerobot.datasets.lerobot_dataset import LeRobotDataset\n",
    "\n",
    "parquet_path = \"/home/joy4mj/Feel2Grasp/train.parquet\"\n",
    "repo_id = \"mjkim00/Feel2Grasp\"\n",
    "revision = \"main\"\n",
    "video_backend = \"pyav\"\n",
    "\n",
    "encoder_ckpt_path = \"./ae_out/encoder.pt\"\n",
    "batch_size = 256\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "out_path = \"replay_buffer_iql_normalized.npz\" \n",
    "EPS = 1e-8 \n",
    "\n",
    "# load encoder\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, 2, 1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((4, 4)),\n",
    "        )\n",
    "        self.fc = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.net(x).flatten(1))\n",
    "\n",
    "try:\n",
    "    ckpt = torch.load(encoder_ckpt_path, map_location=\"cpu\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Encoder checkpoint not found at {encoder_ckpt_path}\")\n",
    "    ckpt = {\"latent_dim\": 64, \"resize_hw\": [128, 128], \"encoder_state_dict\": {}}\n",
    "    print(\"Using dummy encoder settings. Please ensure real encoder is loaded.\")\n",
    "\n",
    "encoder = ConvEncoder(ckpt[\"latent_dim\"])\n",
    "if ckpt[\"encoder_state_dict\"]:\n",
    "    encoder.load_state_dict(ckpt[\"encoder_state_dict\"])\n",
    "encoder.to(device).eval()\n",
    "for p in encoder.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "resize_hw = tuple(ckpt[\"resize_hw\"])\n",
    "\n",
    "#  Parquet load + sanity check\n",
    "\n",
    "df = pd.read_parquet(parquet_path)\n",
    "\n",
    "need_cols = [\n",
    "    \"action\", \"observation.state\", \"episode_index\", \"frame_index\", \"index\",\n",
    "    \"left_image_circle\", \"right_image_circle\", \"circle_reward\"\n",
    "]\n",
    "\n",
    "df = df.sort_values([\"episode_index\", \"frame_index\"]).reset_index(drop=True)\n",
    "N = len(df)\n",
    "print(\"N =\", N)\n",
    "\n",
    "ep = df[\"episode_index\"].to_numpy()\n",
    "fr = df[\"frame_index\"].to_numpy()\n",
    "global_idx = df[\"index\"].to_numpy()\n",
    "\n",
    "# check for duplicated (episode_index, frame_index)\n",
    "obs_state = np.stack(df[\"observation.state\"].to_list()).astype(np.float32)\n",
    "actions   = np.stack(df[\"action\"].to_list()).astype(np.float32)\n",
    "lr01      = df[[\"left_image_circle\", \"right_image_circle\"]].to_numpy().astype(np.float32)\n",
    "rewards   = df[\"circle_reward\"].to_numpy().astype(np.float32)\n",
    "\n",
    "if obs_state.ndim != 2 or obs_state.shape[1] != 6:\n",
    "    raise ValueError(f\"observation.state expected (N,6) but got {obs_state.shape}\")\n",
    "\n",
    "# Reward Rescaling: -1/+10 -> -0.1/+1.0 \n",
    "rewards = rewards / 10.0\n",
    "\n",
    "# LeRobotDataset load\n",
    "ds = LeRobotDataset(repo_id, revision=revision, video_backend=video_backend)\n",
    "print(\"len(ds) =\", len(ds))\n",
    "\n",
    "def get_sample(ds, i: int):\n",
    "    return ds[int(i)]\n",
    "\n",
    "def get_key(sample, key):\n",
    "    if key in sample:\n",
    "        return sample[key]\n",
    "    raise KeyError(f\"Key {key} not found in sample keys: {list(sample.keys())[:30]} ...\")\n",
    "\n",
    "# index mapping\n",
    "check_k = min(200, N)\n",
    "ok_direct = True\n",
    "for k in np.linspace(0, N-1, check_k, dtype=int):\n",
    "    i = int(global_idx[k])\n",
    "    samp = get_sample(ds, i)\n",
    "    ep_ds = int(get_key(samp, \"episode_index\"))\n",
    "    fr_ds = int(get_key(samp, \"frame_index\"))\n",
    "    if ep_ds != int(ep[k]) or fr_ds != int(fr[k]):\n",
    "        ok_direct = False\n",
    "        break\n",
    "\n",
    "if not ok_direct:\n",
    "    print(\"[WARN] parquet.index != ds index mapping. Building (episode,frame)->ds_index map...\")\n",
    "    map_epfr_to_dsidx = {}\n",
    "    for i in range(len(ds)):\n",
    "        samp = get_sample(ds, i)\n",
    "        ep_i = int(get_key(samp, \"episode_index\"))\n",
    "        fr_i = int(get_key(samp, \"frame_index\"))\n",
    "        map_epfr_to_dsidx[(ep_i, fr_i)] = i\n",
    "\n",
    "    ds_indices = np.empty((N,), dtype=np.int64)\n",
    "    for k in range(N):\n",
    "        key = (int(ep[k]), int(fr[k]))\n",
    "        if key not in map_epfr_to_dsidx:\n",
    "            raise KeyError(f\"Cannot find ds index for (episode,frame)={key}\")\n",
    "        ds_indices[k] = map_epfr_to_dsidx[key]\n",
    "else:\n",
    "    ds_indices = global_idx.astype(np.int64)\n",
    "\n",
    "\n",
    "# front image -> encoder -> z(64) \n",
    "@torch.no_grad()\n",
    "def encode_front_batch(ds, idx_batch):\n",
    "    imgs = []\n",
    "    for i in idx_batch:\n",
    "        samp = get_sample(ds, int(i))\n",
    "        img = get_key(samp, \"observation.images.front\")\n",
    "        if isinstance(img, np.ndarray):\n",
    "            if img.ndim == 3 and img.shape[-1] == 3:\n",
    "                img = torch.from_numpy(img).permute(2, 0, 1)\n",
    "            else:\n",
    "                img = torch.from_numpy(img)\n",
    "        \n",
    "        if img.dtype == torch.uint8:\n",
    "            img = img.float() / 255.0\n",
    "        else:\n",
    "            img = img.float()\n",
    "            if img.max() > 1.5:\n",
    "                img = img / 255.0\n",
    "\n",
    "        imgs.append(img)\n",
    "\n",
    "    x = torch.stack(imgs, dim=0).to(device)\n",
    "    x = F.interpolate(x, size=resize_hw, mode=\"bilinear\", align_corners=False)\n",
    "    z = encoder(x)\n",
    "    return z.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "Z = np.empty((N, ckpt[\"latent_dim\"]), dtype=np.float32)\n",
    "\n",
    "for start in range(0, N, batch_size):\n",
    "    end = min(N, start + batch_size)\n",
    "    Z[start:end] = encode_front_batch(ds, ds_indices[start:end])\n",
    "    if (start // batch_size) % 20 == 0:\n",
    "        print(f\"encoded {end}/{N}\")\n",
    "\n",
    "\n",
    "# states(72) \n",
    "states = np.concatenate([Z, lr01, obs_state], axis=1).astype(np.float32)\n",
    "if states.shape[1] != 72:\n",
    "    raise ValueError(f\"states dim expected 72 but got {states.shape}\")\n",
    "\n",
    "# done / next_state \n",
    "terminals = np.zeros((N,), dtype=np.float32)\n",
    "cont = (ep[1:] == ep[:-1]) & (fr[1:] == fr[:-1] + 1)\n",
    "terminals[:-1] = (~cont).astype(np.float32)\n",
    "terminals[-1] = 1.0\n",
    "\n",
    "next_states = np.empty_like(states)\n",
    "next_states[:-1] = states[1:]\n",
    "next_states[-1] = states[-1]\n",
    "\n",
    "terminal_idx = np.where(terminals > 0.5)[0]\n",
    "next_states[terminal_idx] = states[terminal_idx]\n",
    "\n",
    "# state / action normalization\n",
    "# 1. State Normalization \n",
    "obs_mean = states.mean(axis=0, keepdims=True)\n",
    "obs_std = states.std(axis=0, keepdims=True)\n",
    "obs_std = np.where(obs_std < EPS, 1.0, obs_std) \n",
    "\n",
    "normalized_states = (states - obs_mean) / obs_std\n",
    "next_states_norm = (next_states - obs_mean) / obs_std\n",
    "\n",
    "# 2. Action Normalization \n",
    "act_mean = actions.mean(axis=0, keepdims=True)\n",
    "act_std = actions.std(axis=0, keepdims=True)\n",
    "act_std = np.where(act_std < EPS, 1.0, act_std)\n",
    "\n",
    "normalized_actions = (actions - act_mean) / act_std\n",
    "\n",
    "# save normalized replay buffer\n",
    "np.savez_compressed(\n",
    "    out_path,\n",
    "    \n",
    "    # normalized \n",
    "    observations=normalized_states.astype(np.float32),\n",
    "    actions=normalized_actions.astype(np.float32),\n",
    "    next_observations=next_states_norm.astype(np.float32),\n",
    "    \n",
    "    rewards=rewards.astype(np.float32), \n",
    "    terminals=terminals.astype(np.float32),\n",
    "    episode_index=ep.astype(np.int32),\n",
    "    frame_index=fr.astype(np.int32),\n",
    "    ds_index=ds_indices.astype(np.int64),\n",
    "    \n",
    "    obs_mean=obs_mean.astype(np.float32),\n",
    "    obs_std=obs_std.astype(np.float32),\n",
    "    act_mean=act_mean.astype(np.float32),\n",
    "    act_std=act_std.astype(np.float32),\n",
    ")\n",
    "\n",
    "print(\"Saved:\", out_path)\n",
    "print(\"Shapes:\",\n",
    "      \"S\", states.shape,\n",
    "      \"A\", actions.shape,\n",
    "      \"R\", rewards.shape,\n",
    "      \"S'\", next_states.shape,\n",
    "      \"D\", terminals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf14b30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: episode/timestep alignment is consistent.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "d = np.load(\"replay_buffer_iql_72d.npz\")\n",
    "ep = d[\"episode_index\"]\n",
    "fr = d[\"frame_index\"]\n",
    "done = d[\"terminals\"]\n",
    "\n",
    "# check episode/timestep alignment\n",
    "idx = np.where(done[:-1] < 0.5)[0]\n",
    "assert np.all(ep[idx] == ep[idx+1]), \"Found done=0 but episode changes!\"\n",
    "assert np.all(fr[idx+1] == fr[idx] + 1), \"Found done=0 but frame_index not consecutive!\"\n",
    "\n",
    "# done==1 \n",
    "idx = np.where(done[:-1] > 0.5)[0]\n",
    "bad = np.where((ep[idx] == ep[idx+1]) & (fr[idx+1] == fr[idx] + 1))[0]\n",
    "assert len(bad) == 0, \"Found done=1 but next is still a valid continuation!\"\n",
    "\n",
    "print(\"OK: episode/timestep alignment is consistent.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c9372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 53936\n",
      "reward min/max: -0.1 1.0\n",
      "count success: 5932\n",
      "success ratio: 0.10998220112726194\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "d = np.load(\"replay_buffer_iql_normalized.npz\")\n",
    "r = d[\"rewards\"]\n",
    "print(\"N:\", len(r))\n",
    "print(\"reward min/max:\", r.min(), r.max())\n",
    "print(\"count success:\", (r > 0.9).sum())   \n",
    "print(\"success ratio:\", (r > 0.9).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "so101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
